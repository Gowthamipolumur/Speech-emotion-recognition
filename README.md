# Speech-emotion-recognition
Using deep learning and machine learning algorithms, To design an automatic emotion recognition system.

The aim of the project is about the detection of the emotions elicited
by the speaker while talking. As an example, speech produced in a state of fear,
anger, or joy becomes loud and fast, with a higher and wider range in pitch,
whereas emotions such as sadness or tiredness generate slow and low-pitched
speech. Detection of human emotions through voice-pattern and speech-pattern
analysis has many applications such as better assisting human-machine
interactions. In particular, we are presenting a classification model of
emotions elicited by speeches based on deep neural networks (CNNs),
Multilayer Perceptron (MLP) Classification based on acoustic features such as Mel Frequency Cepstral Coefficient
(MFCC).The models have been trained to classify seven different emotions
(neutral, calm, happy, sad, angry, fearful, disgust, surprise). Our evaluation
shows that the proposed approach yields accuracies of 86%, 84% 
using CNN and MLP  respectively, for 7 emotions using Ryerson
Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset
